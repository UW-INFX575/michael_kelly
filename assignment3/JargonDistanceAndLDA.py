import numpy as np
import lda
import lda.datasets
import requests
import nltk
from nltk.util import ngrams
from nltk.corpus import stopwords
from nltk.tokenize import RegexpTokenizer
from nltk.stem.porter import *
import textmining
from collections import Counter
import sys, os

def main():
    print 'LDA on Sample Set of Patent Documents'
    LDA_on_patent_data()
    
    print 'LDA on Toy Data Set of Silly Sentences'
    LDA_on_toy_data()
    
    print 'Jargon Distance on Two Groups of Silly Sentences'
    print '%f Cultural Hole\n' % jargon_distance_on_toy_data()
    
    print 'Jargon Distance between Documents on Cats and Hipsters'
    print '%f Cultural Hole\n' % jargon_distance_cats_hipsters()
    
    print 'Jargon Distance between Documents Both on Cats'
    print '%f Cultural Hole\n' % jargon_distance_cats_cats()


def stop_and_stem(document):
    '''Removes punctuation and stop words; reduces remaining words to stems
    Takes a document (string)
    Returns words (list of strings)'''
    
    tokenizer = RegexpTokenizer(r'[a-z]+') # remove punctuation
    document = document.lower()
    document = tokenizer.tokenize(document)
    
    stop = stopwords.words('english')
    words = [word for word in document if word not in stop]
    
    stemmer = PorterStemmer()
    words = [stemmer.stem(word) for word in words]
    
    words = [x.encode('UTF8') for x in words] # use ASCII
    
    return words


def get_TDM(documents):
    '''Creates term-document matrix
    Takes a set of documents (list of strings)
    Returns the TDM and the unique term list as a tuple'''
    
    tdm = textmining.TermDocumentMatrix()

    for doc in documents:
        tdm.add_doc(doc)

    matrix = np.array([row for row in tdm.rows(cutoff=1)])
    term_list = matrix[0,:]
    term_document_matrix = matrix[1:,:]
    term_document_matrix = term_document_matrix.astype(int)
    
    return (term_document_matrix, term_list)


def run_LDA(term_document_matrix, term_list, topics=20):
    '''Prints topics and their words generated by LDA analysis'''
    
    # LDA function spews unneeded output
    # Supress stdout before calling it
    _stdout = sys.stdout
    _stderr = sys.stderr
    null = open(os.devnull,'wb')
    sys.stdout = null
    sys.stderr = null

    model = lda.LDA(n_topics=topics, n_iter=1500, random_state=1)
    model.fit(term_document_matrix)
    topic_word = model.topic_word_
    n_top_words = 8

    # Allow stdout again
    sys.stdout = _stdout
    sys.stderr = _stderr

    with open('LDA-Results.txt', 'w') as output_file:
        for i, topic_dist in enumerate(topic_word):
            topic_words = np.array(term_list)[np.argsort(topic_dist)][:-n_top_words:-1]
            line = 'Topic {}: {}'.format(i+1, ' '.join(topic_words))
            
            print line
            output_file.write(line + '\n')
    print '\n'
    

def ngram_freq(ngram_list):
    '''Counts the unique occurences of ngrams in a list
    Takes a set of ngrams (list of strings or tuples)
    Returns a sorted list of tuples with frequency count.
    e.g. [('Hey', 'friend')...] --> [(('Hey', 'friend'), 4)...]'''
    
    freq = nltk.FreqDist(ngram_list).items()
    freq = sorted(freq, key=lambda tup: tup[-1], reverse=True)
    return freq


def get_codebooks(field1_ngram_freq, field2_ngram_freq):
    '''Produces codebooks, or phrase frequency distributions, for two fields
    Takes two lists of tuples (ngrams with frequency count)
    Returns tuple of two dicts: a probability distribution codebook for each field'''
    
    alpha = 0.01 

    field1_ngram_freq = dict(field1_ngram_freq)
    field1_ngram_freq = Counter(field1_ngram_freq)
    
    field2_ngram_freq = dict(field2_ngram_freq)
    field2_ngram_freq = Counter(field2_ngram_freq)
    
    corpus_ngram_freq = field1_ngram_freq + field2_ngram_freq
    corpus_alpha_freq = {ngram: alpha for ngram in corpus_ngram_freq}
    
    field1_total_count = sum(field1_ngram_freq.itervalues())
    field2_total_count = sum(field2_ngram_freq.itervalues())
    corpus_total_count = sum(corpus_ngram_freq.itervalues())
    
    field1_prob_dist = {}
    field2_prob_dist = {}
    corpus_prob_dist = {}
    
    # Use non-zero alpha as initial word frequency
    field1_codebook = corpus_alpha_freq.copy()
    field2_codebook = corpus_alpha_freq.copy()
    
    for k, v in corpus_ngram_freq.iteritems():
        corpus_prob_dist[k] = float(v) / corpus_total_count
    
    for k, v in field1_ngram_freq.iteritems():
        prob_dist_k = float(v) / field1_total_count
        field1_codebook[k] = (1 - alpha) * prob_dist_k + alpha * corpus_prob_dist[k]
        
    for k, v in field2_ngram_freq.iteritems():
        prob_dist_k = float(v) / field2_total_count
        field2_codebook[k] = (1 - alpha) * prob_dist_k + alpha * corpus_prob_dist[k]
    
    return (field1_codebook, field2_codebook)


def shannon_entropy(prob_dist):
    '''Computes Shannon entropy of a probability distribution
    Takes a dict: probability distribution of phrases in field
    Returns a float: the entropy / efficiency of jargon within field'''
    
    shannon_entropy = -sum([x * np.log2(x) for x in prob_dist.itervalues()])
    print '%f Shannon Entropy' % shannon_entropy
    return shannon_entropy


def cross_entropy(prob_dist1, prob_dist2):
    '''Computes the cross entropy of two probability distributions
    Takes two dicts with same keys: probability distributions
    Returns a float: the cross entropy'''
    
    x = []
    for k, v in prob_dist1.iteritems():
        x.append(prob_dist1[k] * np.log2(prob_dist2[k]))
    cross_entropy = -sum(x)
    print '%f Cross Entropy' % cross_entropy
    return cross_entropy


def jargon_distance(field1_codebook, field2_codebook):
    '''Computes the jargon distance between two fields
    Takes two dicts: a probability distribution codebook for field 1 and 2
    Returns a float: the cultural hole or communication inefffiency between fields'''
    
    efficiency = shannon_entropy(field1_codebook) / cross_entropy(field1_codebook, field2_codebook)
    cultural_hole = 1 - efficiency
    
    return cultural_hole
    
    
def LDA_on_patent_data():
    '''Fetches patent documents from server and performs LDA using unigrams'''
    
    url = 'https://s3-us-west-2.amazonaws.com/uspto-patentsclaims/'
    ids = range(6334220, 6334230) # Document IDs to fetch
    ids = [str(doc_id) for doc_id in ids]

    documents = {}
    for doc_id in ids:
        documents[doc_id] = requests.get(url + doc_id + '.txt').content
    
    all_unigrams = []
    for document in documents.itervalues():
        words = stop_and_stem(document)
        all_unigrams.append(' '.join(words))
        
    term_document_matrix, term_list = get_TDM(all_unigrams)
    run_LDA(term_document_matrix, term_list)

    
def LDA_on_toy_data():
    '''Runs LDA analysis on a set of 5 simple sentences'''
    
    toy_data = ['I like to eat broccoli and bananas.', 
                'I ate a banana and spinach smoothie for breakfast.', 
               'Chinchillas and kittens are cute.', 
               'My sister adopted a kitten yesterday.',
               'Look at this cute hamster munching on a piece of broccoli.']
    
    documents_in_unigrams = []
    for document in toy_data:
        words = stop_and_stem(document)
        documents_in_unigrams.append(' '.join(words))
    
    term_document_matrix, term_list = get_TDM(documents_in_unigrams)
    run_LDA(term_document_matrix, term_list, 2)


def jargon_distance_on_toy_data():
    '''Computes jargon distance between 2 groups consisting of 5 simple sentences'''
    
    field1 = ['Chinchillas and kittens are cute.', 
              'My sister adopted a kitten yesterday.',
              'Look at this cute hamster munching on a piece of broccoli.']
    field2 = ['I like to eat broccoli and bananas.', 
              'I ate a banana and spinach smoothie for breakfast.']
    
    field1_in_unigrams = []
    for document in field1:
        field1_in_unigrams += stop_and_stem(document)

    field2_in_unigrams = []
    for document in field2:
        field2_in_unigrams += stop_and_stem(document)
    
    field1_unigram_freq = ngram_freq(field1_in_unigrams)
    field2_unigram_freq = ngram_freq(field2_in_unigrams)

    field1_codebook, field2_codebook = get_codebooks(field1_unigram_freq, 
                                                     field2_unigram_freq)
    
    return jargon_distance(field1_codebook, field2_codebook)


def jargon_distance_cats_hipsters():
    '''Computes jargon distance between 2 groups of generated text:
    One full of cat words, the other hipster words.'''
    
    documents = []
    filenames = ['cats1.txt','cats2.txt','cats3.txt', 'hipsum1.txt', 'hipsum2.txt']
    for filename in filenames:
        with open(filename, "r") as myfile:
            documents.append(myfile.read().replace('\n', ''))
    
    field1_in_unigrams = []
    for document in documents[:3]:
        field1_in_unigrams += stop_and_stem(document)

    field2_in_unigrams = []
    for document in documents[3:]:
        field2_in_unigrams += stop_and_stem(document)
    
    field1_unigram_freq = ngram_freq(field1_in_unigrams)
    field2_unigram_freq = ngram_freq(field2_in_unigrams)

    field1_codebook, field2_codebook = get_codebooks(field1_unigram_freq, 
                                                     field2_unigram_freq)
    
    return jargon_distance(field1_codebook, field2_codebook)


def jargon_distance_cats_cats():
    '''Computes jargon distance between 2 groups of generated text:
    both of the same theme! Distance should be small since the groups
    have the same theme with shared words.'''
    
    documents = []
    filenames = ['cats1.txt','cats2.txt']
    for filename in filenames:
        with open(filename, "r") as myfile:
            documents.append(myfile.read().replace('\n', ''))
    
    field1_in_unigrams = []
    for document in documents[:1]:
        field1_in_unigrams += stop_and_stem(document)

    field2_in_unigrams = []
    for document in documents[1:]:
        field2_in_unigrams += stop_and_stem(document)
    
    field1_unigram_freq = ngram_freq(field1_in_unigrams)
    field2_unigram_freq = ngram_freq(field2_in_unigrams)

    field1_codebook, field2_codebook = get_codebooks(field1_unigram_freq, 
                                                     field2_unigram_freq)
    
    return jargon_distance(field1_codebook, field2_codebook)

main()