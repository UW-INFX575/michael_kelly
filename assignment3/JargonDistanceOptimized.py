import numpy as np
import lda
import lda.datasets
import requests
import nltk
from nltk.corpus import stopwords
from nltk.tokenize import RegexpTokenizer
from nltk.stem import PorterStemmer
import textmining
from collections import Counter
import sys, os
import csv

def main():
    stopwords = ["all","just","being","over","both","through","yourselves","its",
                 "before","herself","had","should","to","only","under","ours","has",
                 "do","them","his","very","they","not","during","now","him","nor",
                 "did","this","she","each","further","where","few","because","doing",
                 "some","are","our","ourselves","out","what","for","while","does","above",
                 "between","t","be","we","who","were","here","hers","by","on","about","of",
                 "against","s","or","own","into","yourself","down","your","from","her",
                 "their","there","been","whom","too","themselves","was","until","more",
                 "himself","that","but","don","with","than","those","he","me","myself",
                 "these","up","will","below","can","theirs","my","and","then","is","am",
                 "it","an","as","itself","at","have","in","any","if","again","no","when",
                 "same","how","other","which","you","after","most","such","why","a","off",
                 "i","yours","so","the","having","once"]

    groups = {}

    groupAssignments = {}
    with open('groups.txt','r') as f:
        next(f) # skip headings
        reader=csv.reader(f,delimiter='\t')
        for pid, group in reader:
            groupAssignments[pid] = group

    with open('abstracts.txt','r') as f:
        reader=csv.reader(f,delimiter='\t')
        for pid, text in reader:
            groupID = groupAssignments[pid]
            if text != 'null':
                if groupID in groups:
                    # append the new number to the existing array at this slot
                    groups[groupID].append(text)
                else:
                    # create a new array in this slot
                    groups[groupID] = [text]

    jargon_distance_among_groups(groups, stopwords)

    
def stop_and_stem(document):
    '''Removes punctuation and stop words; reduces remaining words to stems
    Takes a document (string)
    Returns words (list of strings)'''
    
    tokenizer = RegexpTokenizer(r'[a-z]+') # remove punctuation
    document = document.lower()
    document = tokenizer.tokenize(document)
    
    stop = stopwords.words('english')
    words = [word for word in document if word not in stop]
    
    stemmer = PorterStemmer()
    words = [stemmer.stem(word) for word in words]
    
    words = [x.encode('UTF8') for x in words] # use ASCII
    
    return words

def stop_custom_list(document, stoplist):
    '''Removes stop words from list
    Takes a document (string)
    Returns words (list of strings)'''
    
    document = document.split(' ')
    words = [word for word in document if word not in stoplist]
    return words


def get_TDM(documents):
    '''Creates term-document matrix
    Takes a set of documents (list of strings)
    Returns the TDM and the unique term list as a tuple'''
    
    tdm = textmining.TermDocumentMatrix()

    for doc in documents:
        tdm.add_doc(doc)

    matrix = np.array([row for row in tdm.rows(cutoff=1)])
    term_list = matrix[0,:]
    term_document_matrix = matrix[1:,:]
    term_document_matrix = term_document_matrix.astype(int)
    
    return (term_document_matrix, term_list)


def run_LDA(term_document_matrix, term_list, topics=20):
    '''Prints topics and their words generated by LDA analysis'''
    
    # LDA function spews unneeded output
    # Supress stdout before calling it
    _stdout = sys.stdout
    _stderr = sys.stderr
    null = open(os.devnull,'wb')
    sys.stdout = null
    sys.stderr = null

    model = lda.LDA(n_topics=topics, n_iter=1500, random_state=1)
    model.fit(term_document_matrix)
    topic_word = model.topic_word_
    n_top_words = 8

    # Allow stdout again
    sys.stdout = _stdout
    sys.stderr = _stderr

    with open('LDA-Results.txt', 'w') as output_file:
        for i, topic_dist in enumerate(topic_word):
            topic_words = np.array(term_list)[np.argsort(topic_dist)][:-n_top_words:-1]
            line = 'Topic {}: {}'.format(i+1, ' '.join(topic_words))
            
            print line
            output_file.write(line + '\n')
    print '\n'
    

def ngram_freq(ngram_list):
    '''Counts the unique occurences of ngrams in a list
    Takes a set of ngrams (list of strings or tuples)
    Returns a sorted list of tuples with frequency count.
    e.g. [('Hey', 'friend')...] --> [(('Hey', 'friend'), 4)...]'''
    
    freq = nltk.FreqDist(ngram_list).items()
    freq = sorted(freq, key=lambda tup: tup[-1], reverse=True)
    return freq


def get_codebooks(group_ngram_freq, corpus_ngram_freq):
    '''Produces codebooks, or phrase frequency distributions, for two fields
    Takes two lists of tuples (ngrams with frequency count)
    Returns tuple of two dicts: a probability distribution codebook for each field'''
    
    alpha = 0.01

    group_ngram_freq = dict(group_ngram_freq)
    group_ngram_freq = Counter(group_ngram_freq)
    
    corpus_ngram_freq = dict(corpus_ngram_freq)
    corpus_ngram_freq = Counter(corpus_ngram_freq)
    
    corpus_ngram_freq = group_ngram_freq + corpus_ngram_freq
    
    group_total_count = sum(group_ngram_freq.itervalues())
    field2_total_count = sum(corpus_ngram_freq.itervalues())
    corpus_total_count = sum(corpus_ngram_freq.itervalues())
    
    group_prob_dist = {}
    field2_prob_dist = {}
    corpus_prob_dist = {}
    
    for k, v in corpus_ngram_freq.iteritems():
        corpus_prob_dist[k] = float(v) / corpus_total_count
    
    # Use corpus probability * alpha as initial word probability
    group_codebook = {k: v * alpha for k, v in corpus_prob_dist.items()}
    field2_codebook = {k: v * alpha for k, v in corpus_prob_dist.items()}
    
    for k, v in group_ngram_freq.iteritems():
        prob_dist_k = float(v) / group_total_count
        group_codebook[k] = (1 - alpha) * prob_dist_k + alpha * corpus_prob_dist[k]
        
    for k, v in corpus_ngram_freq.iteritems():
        prob_dist_k = float(v) / field2_total_count
        field2_codebook[k] = (1 - alpha) * prob_dist_k + alpha * corpus_prob_dist[k]
    
    return (group_codebook, field2_codebook)


def shannon_entropy(prob_dist):
    '''Computes Shannon entropy of a probability distribution
    Takes a dict: probability distribution of phrases in field
    Returns a float: the entropy / efficiency of jargon within field'''
    
    shannon_entropy = -sum([x * np.log2(x) for x in prob_dist.itervalues()])
    print '%f Shannon Entropy' % shannon_entropy
    return shannon_entropy


def cross_entropy(prob_dist1, prob_dist2):
    '''Computes the cross entropy of two probability distributions
    Takes two dicts with same keys: probability distributions
    Returns a float: the cross entropy'''
    
    x = []
    for k, v in prob_dist1.iteritems():
        x.append(prob_dist1[k] * np.log2(prob_dist2[k]))
    cross_entropy = -sum(x)
    #print '%f Cross Entropy' % cross_entropy
    return cross_entropy


def jargon_distance(group_codebook, field2_codebook):
    '''Computes the jargon distance between two fields
    Takes two dicts: a probability distribution codebook for field 1 and 2
    Returns a float: the cultural hole or communication inefffiency between fields'''
    
    efficiency = shannon_entropy(group_codebook) / cross_entropy(group_codebook, field2_codebook)
    cultural_hole = 1 - efficiency
    
    return cultural_hole
    
    
def LDA_on_patent_data():
    '''Fetches patent documents from server and performs LDA using unigrams'''
    
    url = 'https://s3-us-west-2.amazonaws.com/uspto-patentsclaims/'
    ids = range(6334220, 6334230) # Document IDs to fetch
    ids = [str(doc_id) for doc_id in ids]

    documents = {}
    for doc_id in ids:
        documents[doc_id] = requests.get(url + doc_id + '.txt').content
    
    all_unigrams = []
    for document in documents.itervalues():
        words = stop_and_stem(document)
        all_unigrams.append(' '.join(words))
        
    term_document_matrix, term_list = get_TDM(all_unigrams)
    run_LDA(term_document_matrix, term_list)

    
def LDA_on_toy_data():
    '''Runs LDA analysis on a set of 5 simple sentences'''
    
    toy_data = ['I like to eat broccoli and bananas.', 
                'I ate a banana and spinach smoothie for breakfast.', 
               'Chinchillas and kittens are cute.', 
               'My sister adopted a kitten yesterday.',
               'Look at this cute hamster munching on a piece of broccoli.']
    
    documents_in_unigrams = []
    for document in toy_data:
        words = stop_and_stem(document)
        documents_in_unigrams.append(' '.join(words))
    
    term_document_matrix, term_list = get_TDM(documents_in_unigrams)
    run_LDA(term_document_matrix, term_list, 2)


def jargon_distance_on_toy_data():
    '''Computes jargon distance between 2 groups consisting of 5 simple sentences'''
    
    group = ['Chinchillas and kittens are cute.', 
              'My sister adopted a kitten yesterday.',
              'Look at this cute hamster munching on a piece of broccoli.']
    field2 = ['I like to eat broccoli and bananas.', 
              'I ate a banana and spinach smoothie for breakfast.']
    
    group_in_unigrams = []
    for document in group:
        group_in_unigrams += stop_and_stem(document)

    field2_in_unigrams = []
    for document in field2:
        field2_in_unigrams += stop_and_stem(document)
    
    group_unigram_freq = ngram_freq(group_in_unigrams)
    field2_unigram_freq = ngram_freq(field2_in_unigrams)

    group_codebook, field2_codebook = get_codebooks(group_unigram_freq, 
                                                     field2_unigram_freq)
    
    return jargon_distance(group_codebook, field2_codebook)


def jargon_distance_cats_hipsters():
    '''Computes jargon distance between 2 groups of generated text:
    One full of cat words, the other hipster words.'''
    
    documents = []
    filenames = ['cats1.txt','cats2.txt','cats3.txt', 'hipsum1.txt', 'hipsum2.txt']
    for filename in filenames:
        with open(filename, "r") as myfile:
            documents.append(myfile.read().replace('\n', ''))
    
    group_in_unigrams = []
    for document in documents[:3]:
        group_in_unigrams += stop_and_stem(document)

    field2_in_unigrams = []
    for document in documents[3:]:
        field2_in_unigrams += stop_and_stem(document)
    
    group_unigram_freq = ngram_freq(group_in_unigrams)
    field2_unigram_freq = ngram_freq(field2_in_unigrams)

    group_codebook, field2_codebook = get_codebooks(group_unigram_freq, 
                                                     field2_unigram_freq)
    
    return jargon_distance(group_codebook, field2_codebook)


def jargon_distance_cats_cats():
    '''Computes jargon distance between 2 groups of generated text:
    both of the same theme! Distance should be small since the groups
    have the same theme with shared words.'''
    
    documents = []
    filenames = ['cats1.txt','cats2.txt']
    for filename in filenames:
        with open(filename, "r") as myfile:
            documents.append(myfile.read().replace('\n', ''))
    
    group_in_unigrams = []
    for document in documents[:1]:
        group_in_unigrams += stop_and_stem(document)

    field2_in_unigrams = []
    for document in documents[1:]:
        field2_in_unigrams += stop_and_stem(document)
    
    group_unigram_freq = ngram_freq(group_in_unigrams)
    field2_unigram_freq = ngram_freq(field2_in_unigrams)

    group_codebook, field2_codebook = get_codebooks(group_unigram_freq, 
                                                     field2_unigram_freq)
    
    return jargon_distance(group_codebook, field2_codebook)


def jargon_distance_among_groups(groups, stopwords):
    
    for group_groupID in groups:
        for field2_groupID in groups:

            group_in_unigrams = []
            for document in groups[group_groupID]:
                group_in_unigrams += stop_custom_list(document, stopwords)

            field2_in_unigrams = []
            for document in groups[field2_groupID]:
                field2_in_unigrams += stop_custom_list(document, stopwords)

            group_unigram_freq = ngram_freq(group_in_unigrams)
            field2_unigram_freq = ngram_freq(field2_in_unigrams)

            group_codebook, field2_codebook = get_codebooks(group_unigram_freq, 
                                                         field2_unigram_freq)

            print '%s, %s, %f' % (group_groupID, field2_groupID, jargon_distance(group_codebook, field2_codebook))


main()