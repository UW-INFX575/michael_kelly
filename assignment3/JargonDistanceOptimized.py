import numpy as np
import lda
import lda.datasets
import requests
import nltk
from nltk.corpus import stopwords
from nltk.tokenize import RegexpTokenizer
from nltk.stem import PorterStemmer
from scipy import cluster
import textmining
import sys
import os
import csv
import matplotlib.pylab as plt

def main():

    # print 'Jargon Distance on Two Groups of Silly Sentences'
    # print '%f Cultural Hole\n' % jargon_distance_on_toy_data()
    
    # print 'Jargon Distance between Documents on Cats and Hipsters'
    # print '%f Cultural Hole\n' % jargon_distance_cats_hipsters()

    print 'Jargon Distance between Groups of Abstracts'
    jargon_distance_abstracts()


def jargon_distance_among_groups(groups, stopwords):
    '''Calculates all combinations of jargon distance among a set of groups
    Takes dict where key is group ID (int) and value is codebook for that group
    Returns ndarray of jargon distances where row is writer group and 
    column is reader group'''

    groups_freq = {}
    corpus_unigrams = []

    for gid in groups:
        group_string = ' '.join(groups[gid])
        group_unigrams = stop_custom_list(group_string, stopwords)
        corpus_unigrams += group_unigrams

        groups_freq[gid] = ngram_freq(group_unigrams)

    corpus_freq = ngram_freq(corpus_unigrams)

    groups_codebooks = { group: get_codebook(group_freq, corpus_freq) for group, group_freq in groups_freq.iteritems() }
    groups_shannon = { group: shannon_entropy(codebook) for group, codebook in groups_codebooks.iteritems() }

    matrix_size = len(groups_freq)
    distance_matrix = np.zeros((matrix_size, matrix_size))
    
    for wid in groups_freq:
        for rid in groups_freq:
            jargon_distance = 1 - (groups_shannon[wid] / cross_entropy(groups_codebooks[wid], groups_codebooks[rid]))
            
            r = wid - 1 # Use zero-based numbering for indexing matrix
            c = rid - 1
            distance_matrix[r, c] = jargon_distance

            print '%s, %s, %f' % (wid, rid, jargon_distance)

    return distance_matrix


def symmetrize(matrix):
    return (matrix + matrix.T) / 2

def stop_and_stem(document):
    '''Removes punctuation and stop words; reduces remaining words to stems
    Takes a document (string)
    Returns words (list of strings)'''
    
    tokenizer = RegexpTokenizer(r'[a-z]+') # remove punctuation
    document = document.lower()
    document = tokenizer.tokenize(document)
    
    stop = stopwords.words('english')
    words = [word for word in document if word not in stop]
    
    stemmer = PorterStemmer()
    words = [stemmer.stem(word) for word in words]
    
    words = [x.encode('UTF8') for x in words] # use ASCII
    
    return words


def stop_custom_list(document, stoplist):
    '''Removes stop words from list
    Takes a document (string) and list of stopwords (list of strings)
    Returns words (list of strings)'''
    
    document = document.split(' ')
    words = [word for word in document if word not in stoplist]
    return words


def get_TDM(documents):
    '''Creates term-document matrix
    Takes a set of documents (list of strings)
    Returns the TDM and the unique term list as a tuple'''
    
    tdm = textmining.TermDocumentMatrix()

    for doc in documents:
        tdm.add_doc(doc)

    matrix = np.array([row for row in tdm.rows(cutoff=1)])
    term_list = matrix[0,:]
    term_document_matrix = matrix[1:,:]
    term_document_matrix = term_document_matrix.astype(int)
    
    return (term_document_matrix, term_list)


def run_LDA(term_document_matrix, term_list, topics=20):
    '''Prints topics and their words generated by LDA analysis'''
    
    # LDA function spews unneeded output
    # Supress stdout before calling it
    _stdout = sys.stdout
    _stderr = sys.stderr
    null = open(os.devnull,'wb')
    sys.stdout = null
    sys.stderr = null

    model = lda.LDA(n_topics=topics, n_iter=1500, random_state=1)
    model.fit(term_document_matrix)
    topic_word = model.topic_word_
    n_top_words = 8

    # Allow stdout again
    sys.stdout = _stdout
    sys.stderr = _stderr

    with open('LDA-Results.txt', 'w') as output_file:
        for i, topic_dist in enumerate(topic_word):
            topic_words = np.array(term_list)[np.argsort(topic_dist)][:-n_top_words:-1]
            line = 'Topic {}: {}'.format(i+1, ' '.join(topic_words))
            
            print line
            output_file.write(line + '\n')
    print '\n'
    

def ngram_freq(ngram_list):
    '''Counts the unique occurences of ngrams in a list
    Takes a set of ngrams (list of strings or tuples)
    Returns a list of tuples with frequency count.'''
    
    return nltk.FreqDist(ngram_list).items()


def get_codebook(group_ngram_freq, corpus_ngram_freq):
    '''Produces a codebook, or phrase frequency distributions, for a group
    Takes two lists of tuples (ngrams with frequency), for group and for corpus
    Returns dict: a probability distribution codebook for the group'''
    
    alpha = 0.01

    group_ngram_freq = dict(group_ngram_freq)
    corpus_ngram_freq = dict(corpus_ngram_freq)

    group_total_count = float(sum(group_ngram_freq.itervalues()))
    corpus_total_count = float(sum(corpus_ngram_freq.itervalues()))
    
    corpus_prob_dist = { word: freq / corpus_total_count for word, freq in corpus_ngram_freq.iteritems() }
    
    # Use corpus probability * alpha as initial word probability
    group_codebook = { word: freq * alpha for word, freq in corpus_prob_dist.items() }
    
    for word, freq in group_ngram_freq.iteritems():
        prob_dist_word = freq / group_total_count
        group_codebook[word] = (1 - alpha) * prob_dist_word + alpha * corpus_prob_dist[word]
    
    return group_codebook


def shannon_entropy(prob_dist):
    '''Computes Shannon entropy of a probability distribution
    Takes a dict: probability distribution of phrases in field
    Returns a float: the entropy / efficiency of jargon within field'''
    
    shannon_entropy = -sum([x * np.log2(x) for x in prob_dist.itervalues()])
    #print '%f Shannon Entropy' % shannon_entropy
    return shannon_entropy


def cross_entropy(prob_dist1, prob_dist2):
    '''Computes the cross entropy of two probability distributions
    Takes two dicts with same keys: probability distributions
    Returns a float: the cross entropy'''
    
    x = []
    for k, v in prob_dist1.iteritems():
        x.append(prob_dist1[k] * np.log2(prob_dist2[k]))
    cross_entropy = -sum(x)
    #print '%f Cross Entropy' % cross_entropy
    return cross_entropy


def get_jargon_distance(writer_codebook, reader_codebook):
    '''Computes the jargon distance between two fields
    Takes two dicts: a probability distribution codebook for field 1 and 2
    Returns a float: the cultural hole or communication inefffiency between fields'''
    
    efficiency = shannon_entropy(writer_codebook) / cross_entropy(writer_codebook, reader_codebook)
    cultural_hole = 1 - efficiency
    
    return cultural_hole

    
def LDA_on_patent_data():
    '''Fetches patent documents from server and performs LDA using unigrams'''
    
    url = 'https://s3-us-west-2.amazonaws.com/uspto-patentsclaims/'
    ids = range(6334220, 6334230) # Document IDs to fetch
    ids = [str(doc_id) for doc_id in ids]

    documents = {}
    for doc_id in ids:
        documents[doc_id] = requests.get(url + doc_id + '.txt').content
    
    all_unigrams = []
    for document in documents.itervalues():
        words = stop_and_stem(document)
        all_unigrams.append(' '.join(words))
        
    term_document_matrix, term_list = get_TDM(all_unigrams)
    run_LDA(term_document_matrix, term_list)

    
def LDA_on_toy_data():
    '''Runs LDA analysis on a set of 5 simple sentences'''
    
    toy_data = ['I like to eat broccoli and bananas.', 
                'I ate a banana and spinach smoothie for breakfast.', 
               'Chinchillas and kittens are cute.', 
               'My sister adopted a kitten yesterday.',
               'Look at this cute hamster munching on a piece of broccoli.']
    
    documents_in_unigrams = []
    for document in toy_data:
        words = stop_and_stem(document)
        documents_in_unigrams.append(' '.join(words))
    
    term_document_matrix, term_list = get_TDM(documents_in_unigrams)
    run_LDA(term_document_matrix, term_list, 2)


def jargon_distance_on_toy_data():
    '''Computes jargon distance between 2 groups consisting of 5 simple sentences'''
    
    field1 = ['Chinchillas and kittens are cute.', 
              'My sister adopted a kitten yesterday.',
              'Look at this cute hamster munching on a piece of broccoli.']
    field2 = ['I like to eat broccoli and bananas.', 
              'I ate a banana and spinach smoothie for breakfast.']
    
    field1_unigrams = []
    for document in field1:
        field1_unigrams += stop_and_stem(document)

    field2_unigrams = []
    for document in field2:
        field2_unigrams += stop_and_stem(document)
    
    corpus_unigrams = field1_unigrams + field2_unigrams

    field1_freq = ngram_freq(field1_unigrams)
    field2_freq = ngram_freq(field2_unigrams)
    corpus_freq = ngram_freq(corpus_unigrams)

    field1_codebook = get_codebook(field1_freq, corpus_freq)
    field2_codebook = get_codebook(field2_freq, corpus_freq)
    
    return get_jargon_distance(field1_codebook, field2_codebook)


def jargon_distance_cats_hipsters():
    '''Computes jargon distance between 2 groups of generated text:
    One full of cat words, the other hipster words.'''
    
    documents = []
    filenames = ['cats1.txt','cats2.txt','cats3.txt', 'hipsum1.txt', 'hipsum2.txt']
    for filename in filenames:
        with open(filename, 'r') as myfile:
            documents.append(myfile.read().replace('\n', ''))
    
    field1_unigrams = []
    for document in documents[:3]:
        field1_unigrams += stop_and_stem(document)

    field2_unigrams = []
    for document in documents[3:]:
        field2_unigrams += stop_and_stem(document)

    corpus_unigrams = field1_unigrams + field2_unigrams
    
    field1_freq = ngram_freq(field1_unigrams)
    field2_freq = ngram_freq(field2_unigrams)
    corpus_freq = ngram_freq(corpus_unigrams)

    field1_codebook = get_codebook(field1_freq, corpus_freq)
    field2_codebook = get_codebook(field2_freq, corpus_freq)
    
    return get_jargon_distance(field1_codebook, field2_codebook)


def jargon_distance_abstracts():
    stopwords = ['all','just','being','over','both','through','yourselves','its',
                 'before','herself','had','should','to','only','under','ours','has',
                 'do','them','his','very','they','not','during','now','him','nor',
                 'did','this','she','each','further','where','few','because','doing',
                 'some','are','our','ourselves','out','what','for','while','does','above',
                 'between','t','be','we','who','were','here','hers','by','on','about','of',
                 'against','s','or','own','into','yourself','down','your','from','her',
                 'their','there','been','whom','too','themselves','was','until','more',
                 'himself','that','but','don','with','than','those','he','me','myself',
                 'these','up','will','below','can','theirs','my','and','then','is','am',
                 'it','an','as','itself','at','have','in','any','if','again','no','when',
                 'same','how','other','which','you','after','most','such','why','a','off',
                 'i','yours','so','the','having','once']

    groups = {}
    group_assignments = {}

    with open('groups2.txt','r') as f:
        next(f) # skip headings
        reader=csv.reader(f, delimiter='\t')
        for pid, gid in reader:
            group_assignments[pid] = int(gid)

    with open('abstracts2.txt','r') as f:
        next(f) # skip headings
        reader=csv.reader(f, delimiter='\t')
        for pid, text in reader:
            gid = group_assignments[pid]
            if text != 'null':
                try:
                    # append the new number to the existing array at this slot
                    groups[gid].append(text)
                except KeyError:
                    # create a new array in this slot
                    groups[gid] = [text]

    distances = jargon_distance_among_groups(groups, stopwords)
    distances_symmetrized = symmetrize(distances)
    distances_clustered = cluster.hierarchy.average(distances_symmetrized)
    
    fig = plt.figure(figsize=(8,8))
    cluster.hierarchy.dendrogram(distances_clustered)
    plt.title('Jargon Clustering')
    plt.ylabel('Distance (Jargon Barrier)')
    plt.xlabel('Group ID')
    fig.savefig('dendrogram.png')


main()