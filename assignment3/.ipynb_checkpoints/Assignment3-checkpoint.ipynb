{
 "metadata": {
  "name": "",
  "signature": "sha256:cbdc09b808efbaa37e4fa76bcc0a3892d9b59460f346737304bb20d00662f029"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import numpy as np\n",
      "import lda\n",
      "import lda.datasets\n",
      "import requests\n",
      "import csv\n",
      "import nltk\n",
      "from nltk.util import ngrams\n",
      "from nltk.corpus import stopwords\n",
      "from nltk.tokenize import RegexpTokenizer\n",
      "from nltk.stem.porter import *\n",
      "import textmining\n",
      "from collections import Counter"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 45
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def stop_and_stem(document):\n",
      "    '''Removes punctuation and stop words; reduces remaining words to stems\n",
      "    Takes a document (string)\n",
      "    Returns words (list of strings)'''\n",
      "    \n",
      "    # Convert to lowercase and remove punctuation from text\n",
      "    tokenizer = RegexpTokenizer(r'[a-z]+')\n",
      "    document = document.lower()\n",
      "    document = tokenizer.tokenize(document)\n",
      "    \n",
      "    # Remove stop words\n",
      "    stop = stopwords.words('english')\n",
      "    words = [word for word in document if word not in stop]\n",
      "    \n",
      "    # Reduce words to their stem\n",
      "    stemmer = PorterStemmer()\n",
      "    words = [stemmer.stem(word) for word in words]\n",
      "    \n",
      "    # Function above returns unicode. Return to ASCII.\n",
      "    words = [x.encode('UTF8') for x in words]\n",
      "    \n",
      "    return words\n",
      "\n",
      "\n",
      "def count_unique_ngrams(ngrams):\n",
      "    '''Counts the unique occurences of ngrams in a list\n",
      "    Takes a set of ngrams (list of strings or tuples)\n",
      "    Returns a sorted list of tuples with frequency count.\n",
      "    e.g. [('Hey', 'friend')...] --> [(('Hey', 'friend'), 4)...]'''\n",
      "    \n",
      "    probDist = []\n",
      "    for i in range(0, len(ngrams)):\n",
      "        probdist = cl.prob_classify(test_feats[i][0])\n",
      "        probDist.append(probdist)\n",
      "    \n",
      "    freq = nltk.FreqDist(ngrams).items()\n",
      "    freq = sorted(freq, key=lambda tup: tup[-1], reverse=True)\n",
      "    prob = map(lambda tup: tup[-1]\n",
      "    return freq\n",
      "\n",
      "def compose_codebooks(field1_docs, field2_docs):\n",
      "    '''Prduces codebooks, or phrase frequency distributions, for two fields\n",
      "    Takes two lists of tuples (ngrams with frequency count)'''\n",
      "{k: f(v) for k, v in my_dictionary.items()}\n",
      "    \n",
      "def create_TDM(documents):\n",
      "    '''Creates term-document matrix\n",
      "    Takes a set of documents (list of strings)\n",
      "    Returns the TDM and the unique term list as a tuple'''\n",
      "    \n",
      "    tdm = textmining.TermDocumentMatrix()\n",
      "\n",
      "    for doc in documents:\n",
      "        tdm.add_doc(doc)\n",
      "\n",
      "    matrix = np.array([row for row in tdm.rows(cutoff=1)])\n",
      "    term_list = matrix[0,:]\n",
      "    term_document_matrix = matrix[1:,:]\n",
      "    term_document_matrix = term_document_matrix.astype(int)\n",
      "    \n",
      "    return (term_document_matrix, term_list)\n",
      "\n",
      "\n",
      "def perform_LDA(term_document_matrix, term_list, topics=20):\n",
      "    '''Prints topics and their words generated by LDA analysis'''\n",
      "    \n",
      "    model = lda.LDA(n_topics=topics, n_iter=1500, random_state=1)\n",
      "    model.fit(term_document_matrix)\n",
      "    topic_word = model.topic_word_\n",
      "\n",
      "    n_top_words = 8\n",
      "    for i, topic_dist in enumerate(topic_word):\n",
      "        topic_words = np.array(term_list)[np.argsort(topic_dist)][:-n_top_words:-1]\n",
      "        print('Topic {}: {}'.format(i+1, ' '.join(topic_words)))\n",
      "    print '\\n'\n",
      "\n",
      "\n",
      "def calculate_jargon_distance():\n",
      "    '''Does something'''\n",
      "    return None\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "def run_LDA_on_patent_data():\n",
      "    '''Fetches patent documents from server and performs LDA using unigrams'''\n",
      "    \n",
      "    url = 'https://s3-us-west-2.amazonaws.com/uspto-patentsclaims/'\n",
      "    ids = range(6334220, 6334230) # Document IDs to fetch\n",
      "    ids = [str(doc_id) for doc_id in ids]\n",
      "\n",
      "    documents = {}\n",
      "    for doc_id in ids:\n",
      "        documents[doc_id] = requests.get(url + doc_id + '.txt').content\n",
      "    \n",
      "    documents_in_unigrams = []\n",
      "\n",
      "    for doc_id, document in documents.items():\n",
      "        words = stop_and_stem(document)\n",
      "        documents_in_unigrams.append(' '.join(words))\n",
      "        \n",
      "    term_document_matrix, term_list = create_TDM(documents_in_unigrams)\n",
      "    perform_LDA(term_document_matrix, term_list)\n",
      "\n",
      "    \n",
      "def run_LDA_on_toy_data():\n",
      "    '''Does something'''\n",
      "    \n",
      "    toy_data = ['I like to eat broccoli and bananas.', 'I ate a banana and spinach smoothie for breakfast.', \n",
      "               'Chinchillas and kittens are cute.', 'My sister adopted a kitten yesterday.',\n",
      "               'Look at this cute hamster munching on a piece of broccoli.']\n",
      "    \n",
      "    documents_in_unigrams = []\n",
      "    for document in toy_data:\n",
      "        words = stop_and_stem(document)\n",
      "        documents_in_unigrams.append(' '.join(words))\n",
      "    \n",
      "    term_document_matrix, term_list = create_TDM(documents_in_unigrams)\n",
      "    perform_LDA(term_document_matrix, term_list, 2)\n",
      "\n",
      "\n",
      "def calculate_jargon_distance_on_toy_data():\n",
      "    '''Does something'''\n",
      "    \n",
      "    field1 = ['Chinchillas and kittens are cute.', \n",
      "              'My sister adopted a kitten yesterday.',\n",
      "              'Look at this cute hamster munching on a piece of broccoli.']\n",
      "    field2 = ['I like to eat broccoli and bananas.', \n",
      "              'I ate a banana and spinach smoothie for breakfast.']\n",
      "    \n",
      "    field1_in_unigrams = []\n",
      "    for document in field1:\n",
      "        words = stop_and_stem(document)\n",
      "        field1_in_unigrams.append(' '.join(words))\n",
      "\n",
      "    field2_in_unigrams = []\n",
      "    for document in field2:\n",
      "        words = stop_and_stem(document)\n",
      "        field2_in_unigrams.append(' '.join(words))\n",
      "    \n",
      "    term_document_matrix, term_list = create_TDM(documents_in_unigrams)\n",
      "    perform_LDA(term_document_matrix, term_list, 2)\n",
      "\n",
      "# Main Sequence\n",
      "#print 'LDA on Sample Set of Patent Documents'\n",
      "#run_LDA_on_patent_data()\n",
      "#print 'LDA on Toy Data Set of Silly Sentences'\n",
      "#run_LDA_on_toy_data()\n",
      "\n",
      "calculate_jargon_distance_on_toy_data()\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Topic 1: broccoli kitten cute hamster look piec yesterday\n",
        "Topic 2: banana eat breakfast chinchilla ate smoothi spinach\n",
        "\n",
        "\n"
       ]
      }
     ],
     "prompt_number": 43
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Shannon entropy\n",
      "# Kullback\u2013Leibler divergence\n",
      "\n",
      "from entropy import shannon_entropy\n",
      "\n",
      "documents = []\n",
      "filenames = ['cats1.txt','cats2.txt','cats3.txt', 'lebowski1.txt', 'lebowski2.txt']\n",
      "for filename in filenames:\n",
      "    with open(filename, \"r\") as myfile:\n",
      "        documents.append(myfile.read().replace('\\n', ''))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 11
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import scipy.stats.distributions\n",
      "import numpy as np"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 16
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "a = np.random.rand(2, 3)\n",
      "scipy.stats.distributions.entropy(a)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 19,
       "text": [
        "array([ 0.69105568,  0.03435817,  0.51501265])"
       ]
      }
     ],
     "prompt_number": 19
    }
   ],
   "metadata": {}
  }
 ]
}