{
 "metadata": {
  "name": "",
  "signature": "sha256:8b89a2c288554fdd31c0222ad8a98dde8b8b0b1208a004dbe9842bd59fb6cee0"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import numpy as np\n",
      "import lda\n",
      "import lda.datasets"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 54
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import requests\n",
      "import csv\n",
      "import nltk\n",
      "from nltk.util import ngrams\n",
      "from nltk.corpus import stopwords\n",
      "from nltk.tokenize import RegexpTokenizer\n",
      "from nltk.stem.porter import *\n",
      "\n",
      "# Retrieve documents from server\n",
      "url = 'https://s3-us-west-2.amazonaws.com/uspto-patentsclaims/'\n",
      "ids = range(6334220, 6334230) # range of document IDs to fetch\n",
      "\n",
      "# Coerce numbers to strings for use as dict keys\n",
      "ids = [str(doc_id) for doc_id in ids]\n",
      "\n",
      "# Store contents of each document in dictionary\n",
      "documents = {}\n",
      "for doc_id in ids:\n",
      "    documents[doc_id] = requests.get(url + doc_id + '.txt').content\n",
      "\n",
      "    # Prepares a document for word analysis by removing punctuation\n",
      "# and stop words, and then reducing remaining words to their stems\n",
      "# Takes a document (string) and returns words (list of strings)\n",
      "def stop_and_stem(document):\n",
      "    # Convert to lowercase and remove punctuation from text\n",
      "    tokenizer = RegexpTokenizer(r'[a-z]+')\n",
      "    document = document.lower()\n",
      "    document = tokenizer.tokenize(document)\n",
      "    \n",
      "    # Remove stop words\n",
      "    stop = stopwords.words('english')\n",
      "    words = [word for word in document if word not in stop]\n",
      "    \n",
      "    # Reduce words to their stem (truncate suffix)\n",
      "    stemmer = PorterStemmer()\n",
      "    words = [stemmer.stem(word) for word in words]\n",
      "    \n",
      "    # Function above returns unicode. Return to ASCII.\n",
      "    words = [x.encode('UTF8') for x in words]\n",
      "    \n",
      "    return words\n",
      "\n",
      "# Counts the unique occurences of ngrams in a list\n",
      "# Takes a set of ngrams (list of strings or tuples)\n",
      "# and returns a sorted list of tuples with frequency count.\n",
      "# e.g. [('Hey', 'friend')...] --> [(('Hey', 'friend'), 4)...]\n",
      "def count_unique_ngrams(ngrams):\n",
      "    freq = nltk.FreqDist(ngrams).items()\n",
      "    freq = sorted(freq, key=lambda tup: tup[-1], reverse=True)\n",
      "    return freq\n",
      "\n",
      "\n",
      "# Write the data to a local file\n",
      "# Takes the name of the file to be written (string)\n",
      "# and the data to be written (list of tuples).  \n",
      "def write_count_file(file_name, data):\n",
      "    with open('output/' + file_name + '.csv', 'w') as write_file:\n",
      "        writer = csv.writer(write_file)\n",
      "        for ngram, count in data:\n",
      "            # for cleaner display, print ngram tuples as plain text\n",
      "            if isinstance(ngram, tuple):\n",
      "                ngram = ' '.join(map(str, ngram))\n",
      "            values = [ngram, count]\n",
      "            writer.writerow(values)\n",
      "\n",
      "# In order to aggregate ngrams across all documents      \n",
      "unigrams_agg = []\n",
      "unigrams_doc = []\n",
      "\n",
      "# Process each document\n",
      "for doc_id, document in documents.items():\n",
      "    # Remove punctuation, stop words, and stem\n",
      "    words = stop_and_stem(document)\n",
      "    \n",
      "    # Hold the ngrams\n",
      "    unigrams_doc.append(' '.join(words))\n",
      "    \n",
      "    # Add ngrams to the aggregates\n",
      "    unigrams_agg += unigrams\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 18
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import textmining\n",
      "\n",
      "# Initialize class to create term-document matrix\n",
      "tdm = textmining.TermDocumentMatrix()\n",
      "# Add the documents\n",
      "for doc in unigrams_doc:\n",
      "    tdm.add_doc(doc)\n",
      "\n",
      "# Write out the matrix to a csv file. Note that setting cutoff=1 means\n",
      "# that words which appear in 1 or more documents will be included in\n",
      "# the output (i.e. every word will appear in the output). The default\n",
      "# for cutoff is 2, since we usually aren't interested in words which\n",
      "# appear in a single document. For this example we want to see all\n",
      "# words however, hence cutoff=1.\n",
      "# tdm.write_csv('matrix.csv', cutoff=1)\n",
      "\n",
      "\n",
      "matrix_full = np.array([row for row in tdm.rows(cutoff=1)])\n",
      "vocab = matrix_full[0,:]\n",
      "matrix_data = matrix_full[1:,:]\n",
      "matrix_data = matrix_data.astype(int)\n",
      "\n",
      "np.shape(matrix_data)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 57,
       "text": [
        "(10, 410)"
       ]
      }
     ],
     "prompt_number": 57
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "model = lda.LDA(n_topics=20, n_iter=1500, random_state=1)\n",
      "model.fit(matrix_data)\n",
      "topic_word = model.topic_word_\n",
      "\n",
      "n_top_words = 8\n",
      "for i, topic_dist in enumerate(topic_word):\n",
      "    topic_words = np.array(vocab)[np.argsort(topic_dist)][:-n_top_words:-1]\n",
      "    print('Topic {}: {}'.format(i, ' '.join(topic_words)))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Topic 0: rotari jet water recit rotat tube apertur\n",
        "Topic 1: side forth set bottom upper made fill\n",
        "Topic 2: said edg second gener end panel protect\n",
        "Topic 3: bathtub rail inner mean assembl compris brace\n",
        "Topic 4: portion cover heel toe ankl foot five\n",
        "Topic 5: end cover front support side open user\n",
        "Topic 6: defin semiconductor roller apparatu edg rotat direct\n",
        "Topic 7: support wherein surfac mount provid engag contact\n",
        "Topic 8: first protector second strap posit section surfac\n",
        "Topic 9: portion bag sleep leg provid part substanti\n",
        "Topic 10: member said faucet portion fix prevent posit\n",
        "Topic 11: wall seat togeth rigid wheel person semirigid\n",
        "Topic 12: claim said bodi one attach main water\n",
        "Topic 13: first abras scrubber speed wafer second clean\n",
        "Topic 14: cm accord measur sleep surfac flow rate\n",
        "Topic 15: claim wherein compris materi first piec vinyl\n",
        "Topic 16: wherein portion least includ two one form\n",
        "Topic 17: first member perineum devic side adjac attach\n",
        "Topic 18: plural tighten defin seat base connector hole\n",
        "Topic 19: said assembl nozzl whirlpool air flow configur\n"
       ]
      }
     ],
     "prompt_number": 49
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "np.shape(matrix_data)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 53,
       "text": [
        "(10, 410)"
       ]
      }
     ],
     "prompt_number": 53
    }
   ],
   "metadata": {}
  }
 ]
}